{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f838d1c3",
   "metadata": {},
   "source": [
    "# Error-aware Negative-enhanced Ranking (ENR) framework for Biological Process concept in Gene Ontology (GO) and Phenotypic Abnormality concept in Human Phenotype Ontology (HPO) recognition — Offline Inference Notebook\n",
    "\n",
    "This notebook runs a **bi-encoder retrieval + cross-encoder reranker** pipeline on local text.\n",
    "\n",
    "You can:\n",
    "- Tag a **single passage** (a string), or\n",
    "- Tag **many passages from a file** (txt/json/jsonl)\n",
    "\n",
    "It outputs predicted **GO-Biological Process** and/or **HPO-Phenotypic Abnormality** concepts per passage.\n",
    "\n",
    "---\n",
    "\n",
    "## What you need\n",
    "\n",
    "1. **Hugging Face model repos**\n",
    "   - Bi-encoder checkpoint (HF `AutoModel`) used for retrieval\n",
    "   - Cross-encoder checkpoint (HF `AutoModelForSequenceClassification`, `num_labels=1`) used for rerank\n",
    "2. **Concept catalogs** (GO and/or HPO) with `id` + `name`\n",
    "   - JSON dict: `{ \"GO_...\": {\"name\":\"...\"}, ... }`\n",
    "   - JSON list: `[{\"id\":\"...\", \"name\":\"...\"}, ...]`\n",
    "   - JSONL: each line `{\"id\":\"...\", \"name\":\"...\"}`\n",
    "\n",
    "> Tip:  You can get catalog files refer to `docs/data.md`. The `data/mm-go/meta/biological_process_concept.json` and `data/mm-hpo/meta/phenotypic_abnormality_concept.json` are valid catalog files.\n",
    "\n",
    "---\n",
    "\n",
    "## Notes on speed\n",
    "\n",
    "- GO/HPO catalogs can be large. We compute and cache concept embeddings once per session.\n",
    "- If `faiss` is available, retrieval is faster. Notebook will fall back to a pure PyTorch implementation otherwise. We do not use `faiss` in this notebook to avoid FAISS segfault.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01068e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in a fresh environment, uncomment:\n",
    "# !pip -q install transformers accelerate sentencepiece huggingface_hub tqdm\n",
    "\n",
    "# Optional (recommended for large catalogs):\n",
    "# !pip -q install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad132d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd742c",
   "metadata": {},
   "source": [
    "## 1) Configure models, catalogs, and inference parameters\n",
    "\n",
    "Fill in your own Hugging Face repo IDs (or local paths).  \n",
    "You may set either GO, HPO, or both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608b445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# User configuration\n",
    "# -----------------------------\n",
    "\n",
    "# Bi-encoder (retrieval) checkpoints\n",
    "GO_BIENCODER = \"Samantha633/enr-recognizer-biological-process-retriever\"     \n",
    "HPO_BIENCODER = \"Samantha633/enr-recognizer-phenotypic-abnormality-retriever\"   \n",
    "\n",
    "# Cross-encoder (rerank) checkpoints\n",
    "GO_CROSSENCODER = \"Samantha633/enr-recognizer-biological-process-reranker\"     \n",
    "HPO_CROSSENCODER = \"Samantha633/enr-recognizer-phenotypic-abnormality-reranker\"   \n",
    "\n",
    "# Concept catalogs (local paths OR files you download beforehand)\n",
    "GO_CATALOG_PATH = \"data/mm-go/meta/biological_process_concept.json\"       # json / jsonl\n",
    "HPO_CATALOG_PATH = \"data/mm-hpo/meta/phenotypic_abnormality_concept.json\"     # json / jsonl\n",
    "\n",
    "# Retrieval / rerank params\n",
    "RETRIEVE_TOPK = 100            # candidates from bi-encoder\n",
    "RERANK_TOPK = 100              # rerank up to this many\n",
    "FINAL_TOPN = 20                # return top-N predictions per passage\n",
    "\n",
    "# Optional: apply a fixed threshold after reranking (if None, use FINAL_TOPN only)\n",
    "GO_SCORE_THRESHOLD = 7.3111958329057485       # e.g. 7.3111958329057485\n",
    "HPO_SCORE_THRESHOLD = 5.729438811082113      # e.g. 5.729438811082113\n",
    "\n",
    "# Tokenization / batching\n",
    "Q_MAX_LEN = 448\n",
    "T_MAX_LEN = 64\n",
    "ENCODE_BS = 64                 # embedding batches\n",
    "RERANK_BS = 64                 # rerank batches\n",
    "\n",
    "# Compute device\n",
    "GPU_ID = 0\n",
    "DEVICE = torch.device(f\"cuda:{GPU_ID}\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd5fb01",
   "metadata": {},
   "source": [
    "## 2) Utilities: IO, normalization, catalog loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727aa776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: Union[str, Path]) -> Any:\n",
    "    path = Path(path)\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def load_jsonl(path: Union[str, Path]) -> List[dict]:\n",
    "    path = Path(path)\n",
    "    rows: List[dict] = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Concept:\n",
    "    cid: str\n",
    "    name: str\n",
    "\n",
    "\n",
    "def load_concept_catalog(path: Union[str, Path]) -> List[Concept]:\n",
    "    \"\"\"Load concept catalog with concept_id + name.\n",
    "\n",
    "    Supports:\n",
    "      - JSONL: each line {\"id\":..., \"name\":...}\n",
    "      - JSON dict: {id: {\"name\":...}, ...}\n",
    "      - JSON list: [{\"id\":..., \"name\":...}, ...]\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    concepts: List[Concept] = []\n",
    "    seen: set[str] = set()\n",
    "\n",
    "    if path.suffix.lower() == \".jsonl\":\n",
    "        for r in load_jsonl(path):\n",
    "            if not isinstance(r, dict):\n",
    "                continue\n",
    "            cid = r.get(\"id\") or r.get(\"concept_id\") or r.get(\"term_id\")\n",
    "            name = r.get(\"name\") or r.get(\"term_text\") or r.get(\"label\")\n",
    "            if cid is None or name is None:\n",
    "                continue\n",
    "            cid = str(cid).strip()\n",
    "            name = str(name).strip()\n",
    "            if cid and name and cid not in seen:\n",
    "                seen.add(cid)\n",
    "                concepts.append(Concept(cid=cid, name=name))\n",
    "        return concepts\n",
    "\n",
    "    obj = load_json(path)\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            cid = str(k).strip()\n",
    "            if not cid or cid in seen:\n",
    "                continue\n",
    "            name = None\n",
    "            if isinstance(v, dict):\n",
    "                name = v.get(\"name\") or v.get(\"term_text\") or v.get(\"label\")\n",
    "            if name is None:\n",
    "                continue\n",
    "            name = str(name).strip()\n",
    "            if name:\n",
    "                seen.add(cid)\n",
    "                concepts.append(Concept(cid=cid, name=name))\n",
    "        return concepts\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        for r in obj:\n",
    "            if not isinstance(r, dict):\n",
    "                continue\n",
    "            cid = r.get(\"id\") or r.get(\"concept_id\") or r.get(\"term_id\")\n",
    "            name = r.get(\"name\") or r.get(\"term_text\") or r.get(\"label\")\n",
    "            if cid is None or name is None:\n",
    "                continue\n",
    "            cid = str(cid).strip()\n",
    "            name = str(name).strip()\n",
    "            if cid and name and cid not in seen:\n",
    "                seen.add(cid)\n",
    "                concepts.append(Concept(cid=cid, name=name))\n",
    "        return concepts\n",
    "\n",
    "    raise ValueError(f\"Unsupported catalog format: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104d6b7",
   "metadata": {},
   "source": [
    "## 3) Utilities: bi-encoder embeddings + retrieval (FAISS optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    mask = attention_mask.unsqueeze(-1)  # [B, L, 1]\n",
    "    summed = (last_hidden_state * mask).sum(dim=1)  # [B, H]\n",
    "    denom = mask.sum(dim=1).clamp(min=1e-9)  # [B, 1]\n",
    "    return summed / denom\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_texts(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    texts: Sequence[str],\n",
    "    max_len: int,\n",
    "    device: torch.device,\n",
    "    batch_size: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Return L2-normalized embeddings on CPU float32.\"\"\"\n",
    "    vecs: List[torch.Tensor] = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\", leave=False):\n",
    "        chunk = list(texts[i : i + batch_size])\n",
    "        enc = tokenizer(chunk, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
    "        out = model(**enc).last_hidden_state\n",
    "        pooled = mean_pooling(out, enc[\"attention_mask\"])\n",
    "        pooled = F.normalize(pooled, dim=-1)\n",
    "        vecs.append(pooled.detach().cpu())\n",
    "    return torch.cat(vecs, dim=0).float() if vecs else torch.empty(0)\n",
    "\n",
    "\n",
    "def _try_build_faiss_index(vectors: np.ndarray, use_gpu: bool, gpu_id: int):\n",
    "    try:\n",
    "        import faiss  # type: ignore\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    dim = vectors.shape[1]\n",
    "    cpu_index = faiss.IndexFlatIP(dim)\n",
    "    cpu_index.add(vectors)\n",
    "\n",
    "    if not use_gpu:\n",
    "        return faiss, cpu_index\n",
    "\n",
    "    if not hasattr(faiss, \"StandardGpuResources\"):\n",
    "        return faiss, cpu_index\n",
    "\n",
    "    try:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        gpu_index = faiss.index_cpu_to_gpu(res, gpu_id, cpu_index)\n",
    "        return faiss, gpu_index\n",
    "    except Exception:\n",
    "        return faiss, cpu_index\n",
    "\n",
    "\n",
    "def retrieve_topk(\n",
    "    term_vecs: torch.Tensor,   # [Nt, D] CPU\n",
    "    query_vecs: torch.Tensor,  # [Nq, D] CPU\n",
    "    topk: int,\n",
    "    use_faiss: bool = True,\n",
    "    use_faiss_gpu: bool = False,\n",
    "    faiss_gpu_id: int = 0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return (scores, indices) with shape [Nq, topk].\"\"\"\n",
    "    term_np = term_vecs.numpy().astype(np.float32, copy=False)\n",
    "    query_np = query_vecs.numpy().astype(np.float32, copy=False)\n",
    "\n",
    "    if use_faiss:\n",
    "        try:\n",
    "            faiss_mod, index = _try_build_faiss_index(term_np, use_gpu=use_faiss_gpu, gpu_id=faiss_gpu_id)\n",
    "            if index is not None:\n",
    "                # 可選：限制 faiss thread，降低 native crash 機率\n",
    "                if faiss_mod is not None and hasattr(faiss_mod, \"omp_set_num_threads\"):\n",
    "                    faiss_mod.omp_set_num_threads(1)\n",
    "\n",
    "                scores, indices = index.search(query_np, int(topk))\n",
    "                return scores, indices\n",
    "        except Exception as e:\n",
    "            print(f\"[FAISS] failed -> fallback to torch. err={repr(e)}\")\n",
    "\n",
    "    sims = torch.from_numpy(query_np) @ torch.from_numpy(term_np).t()\n",
    "    vals, idxs = torch.topk(sims, k=min(int(topk), sims.size(1)), dim=1, largest=True, sorted=True)\n",
    "    return vals.numpy(), idxs.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff4ef4",
   "metadata": {},
   "source": [
    "## 4) Utilities: cross-encoder rerank\n",
    "\n",
    "We score pairs `(term_text, query_text)` (same direction as your eval scripts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dda0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def rerank_with_cross_encoder(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    query_texts: List[str],\n",
    "    candidate_texts: List[List[str]],\n",
    "    max_len: int,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"Return per-query score arrays aligned with candidate_texts.\"\"\"\n",
    "    model.eval()\n",
    "    out_scores: List[np.ndarray] = []\n",
    "\n",
    "    for q, cands in tqdm(list(zip(query_texts, candidate_texts)), desc=\"Reranking\"):\n",
    "        if not cands:\n",
    "            out_scores.append(np.zeros((0,), dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        scores_chunks: List[np.ndarray] = []\n",
    "        for i in range(0, len(cands), batch_size):\n",
    "            chunk = cands[i : i + batch_size]\n",
    "            enc = tokenizer(\n",
    "                chunk,\n",
    "                [q] * len(chunk),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
    "            logits = model(**enc).logits.squeeze(-1)\n",
    "            scores_chunks.append(logits.detach().cpu().numpy())\n",
    "        out_scores.append(np.concatenate(scores_chunks, axis=0) if scores_chunks else np.zeros((0,), dtype=np.float32))\n",
    "\n",
    "    return out_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd8699",
   "metadata": {},
   "source": [
    "## 5) Build a reusable tagger class (GO or HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TaggerConfig:\n",
    "    biencoder_name_or_path: str\n",
    "    crossencoder_name_or_path: Optional[str]\n",
    "    catalog_path: str\n",
    "    retrieve_topk: int = 100\n",
    "    rerank_topk: int = 100\n",
    "    final_topn: int = 20\n",
    "    score_threshold: Optional[float] = None\n",
    "    q_max_len: int = 448\n",
    "    t_max_len: int = 64\n",
    "    encode_bs: int = 64\n",
    "    rerank_bs: int = 64\n",
    "    use_faiss: bool = True\n",
    "    faiss_gpu: bool = False\n",
    "    gpu_id: int = 0\n",
    "\n",
    "\n",
    "class ConceptTagger:\n",
    "    def __init__(self, cfg: TaggerConfig, device: torch.device):\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "\n",
    "        # Load catalog\n",
    "        self.concepts: List[Concept] = load_concept_catalog(cfg.catalog_path)\n",
    "        if not self.concepts:\n",
    "            raise ValueError(f\"Empty catalog: {cfg.catalog_path}\")\n",
    "        self.term_texts = [c.name for c in self.concepts]\n",
    "\n",
    "        # Load bi-encoder\n",
    "        self.bi_tok = AutoTokenizer.from_pretrained(cfg.biencoder_name_or_path, use_fast=True)\n",
    "        self.bi_model = AutoModel.from_pretrained(cfg.biencoder_name_or_path).to(device)\n",
    "        self.bi_model.eval()\n",
    "\n",
    "        # Load cross-encoder (optional)\n",
    "        self.ce_tok = None\n",
    "        self.ce_model = None\n",
    "        if cfg.crossencoder_name_or_path:\n",
    "            self.ce_tok = AutoTokenizer.from_pretrained(cfg.crossencoder_name_or_path, use_fast=True)\n",
    "            self.ce_model = AutoModelForSequenceClassification.from_pretrained(cfg.crossencoder_name_or_path).to(device)\n",
    "            self.ce_model.eval()\n",
    "\n",
    "        # Cache for concept embeddings (CPU tensor)\n",
    "        self._term_vecs: Optional[torch.Tensor] = None\n",
    "\n",
    "    def build_index(self) -> None:\n",
    "        \"\"\"Compute and cache term embeddings (CPU).\"\"\"\n",
    "        if self._term_vecs is not None:\n",
    "            return\n",
    "        print(f\"[Index] Encoding {len(self.term_texts):,} concepts...\")\n",
    "        self._term_vecs = encode_texts(\n",
    "            self.bi_model, self.bi_tok,\n",
    "            self.term_texts,\n",
    "            max_len=self.cfg.t_max_len,\n",
    "            device=self.device,\n",
    "            batch_size=self.cfg.encode_bs,\n",
    "        )\n",
    "        print(\"[Index] Done.\")\n",
    "\n",
    "    def _retrieve(self, query_texts: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        self.build_index()\n",
    "        assert self._term_vecs is not None\n",
    "\n",
    "        print(f\"[Retrieve] Encoding {len(query_texts)} queries...\")\n",
    "        q_vecs = encode_texts(\n",
    "            self.bi_model, self.bi_tok,\n",
    "            query_texts,\n",
    "            max_len=self.cfg.q_max_len,\n",
    "            device=self.device,\n",
    "            batch_size=self.cfg.encode_bs,\n",
    "        )\n",
    "        scores, indices = retrieve_topk(\n",
    "            term_vecs=self._term_vecs,\n",
    "            query_vecs=q_vecs,\n",
    "            topk=self.cfg.retrieve_topk,\n",
    "            use_faiss=self.cfg.use_faiss,\n",
    "            use_faiss_gpu=(self.cfg.faiss_gpu and torch.cuda.is_available()),\n",
    "            faiss_gpu_id=self.cfg.gpu_id,\n",
    "        )\n",
    "        return scores, indices\n",
    "\n",
    "    def predict(self, passages: List[str]) -> List[dict]:\n",
    "        \"\"\"Return per-passage predictions with term_id and term_text.\n",
    "\n",
    "        Output per passage:\n",
    "          {\"query_text\": \"...\",\n",
    "           \"items\": [{\"rank\":1,\"score\":...,\"term_id\":\"...\",\"term_text\":\"...\"}, ...]}\n",
    "        Score is rerank score if cross-encoder present else bi-encoder similarity.\n",
    "        \"\"\"\n",
    "        passages = [p for p in passages if isinstance(p, str) and p.strip()]\n",
    "        if not passages:\n",
    "            return []\n",
    "\n",
    "        # 1) bi-encoder retrieval\n",
    "        ret_scores, ret_indices = self._retrieve(passages)\n",
    "\n",
    "        # Build candidate lists\n",
    "        cand_texts: List[List[str]] = []\n",
    "        cand_ids: List[List[str]] = []\n",
    "        for idx_row in ret_indices:\n",
    "            ids = []\n",
    "            texts = []\n",
    "            for j in idx_row.tolist():\n",
    "                j = int(j)\n",
    "                if j < 0 or j >= len(self.concepts):\n",
    "                    continue\n",
    "                c = self.concepts[j]\n",
    "                ids.append(c.cid)\n",
    "                texts.append(c.name)\n",
    "            cand_ids.append(ids[: self.cfg.rerank_topk])\n",
    "            cand_texts.append(texts[: self.cfg.rerank_topk])\n",
    "\n",
    "        # 2) rerank (optional)\n",
    "        if self.ce_model is not None and self.ce_tok is not None:\n",
    "            ce_scores = rerank_with_cross_encoder(\n",
    "                tokenizer=self.ce_tok,\n",
    "                model=self.ce_model,\n",
    "                query_texts=passages,\n",
    "                candidate_texts=cand_texts,\n",
    "                max_len=self.cfg.q_max_len,\n",
    "                batch_size=self.cfg.rerank_bs,\n",
    "                device=self.device,\n",
    "            )\n",
    "            final_scores = ce_scores\n",
    "        else:\n",
    "            final_scores = [np.asarray(s[: self.cfg.rerank_topk], dtype=np.float32) for s in ret_scores]\n",
    "\n",
    "        # 3) sort + filter\n",
    "        outputs: List[dict] = []\n",
    "        for q, ids, texts, scores in zip(passages, cand_ids, cand_texts, final_scores):\n",
    "            if len(scores) != len(texts):\n",
    "                m = min(len(scores), len(texts))\n",
    "                ids, texts, scores = ids[:m], texts[:m], scores[:m]\n",
    "\n",
    "            order = np.argsort(-scores)\n",
    "            items = []\n",
    "            for rank1, ii in enumerate(order.tolist(), start=1):\n",
    "                items.append({\n",
    "                    \"rank\": rank1,\n",
    "                    \"score\": float(scores[ii]),\n",
    "                    \"term_id\": ids[ii],\n",
    "                    \"term_text\": texts[ii],\n",
    "                })\n",
    "\n",
    "            if self.cfg.score_threshold is not None:\n",
    "                items = [it for it in items if float(it[\"score\"]) >= float(self.cfg.score_threshold)]\n",
    "\n",
    "            if self.cfg.final_topn is not None and self.cfg.final_topn > 0:\n",
    "                items = items[: int(self.cfg.final_topn)]\n",
    "\n",
    "            outputs.append({\"query_text\": q, \"items\": items})\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f3d4b",
   "metadata": {},
   "source": [
    "## 6) Initialize taggers (GO / HPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd74f8",
   "metadata": {},
   "source": [
    "## 7) Input: single passage OR a file with multiple passages\n",
    "\n",
    "Supported file formats:\n",
    "- `.txt`: one passage per non-empty line\n",
    "- `.jsonl`: each line a dict with key `passage` or `query_text` or `text`\n",
    "- `.json`: list of strings or list of dicts with `passage`/`query_text`/`text`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_passages_from_file(path: Union[str, Path], limit: Optional[int] = None) -> List[str]:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    passages: List[str] = []\n",
    "\n",
    "    if path.suffix.lower() == \".txt\":\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                s = line.strip()\n",
    "                if s:\n",
    "                    passages.append(s)\n",
    "                    if limit is not None and len(passages) >= limit:\n",
    "                        break\n",
    "        return passages\n",
    "\n",
    "    if path.suffix.lower() == \".jsonl\":\n",
    "        for r in load_jsonl(path):\n",
    "            if not isinstance(r, dict):\n",
    "                continue\n",
    "            s = r.get(\"passage\") or r.get(\"query_text\") or r.get(\"text\")\n",
    "            if isinstance(s, str) and s.strip():\n",
    "                passages.append(s.strip())\n",
    "                if limit is not None and len(passages) >= limit:\n",
    "                    break\n",
    "        return passages\n",
    "\n",
    "    if path.suffix.lower() == \".json\":\n",
    "        obj = load_json(path)\n",
    "        if isinstance(obj, list):\n",
    "            for r in obj:\n",
    "                if isinstance(r, str) and r.strip():\n",
    "                    passages.append(r.strip())\n",
    "                elif isinstance(r, dict):\n",
    "                    s = r.get(\"passage\") or r.get(\"query_text\") or r.get(\"text\")\n",
    "                    if isinstance(s, str) and s.strip():\n",
    "                        passages.append(s.strip())\n",
    "                if limit is not None and len(passages) >= limit:\n",
    "                    break\n",
    "            return passages\n",
    "\n",
    "    raise ValueError(f\"Unsupported file format: {path}\")\n",
    "\n",
    "\n",
    "# Example: single passage\n",
    "passages = [\n",
    "    \"Nonylphenol and short-chain nonylphenol ethoxylates such as NP2 EO are present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. © 2015 Wiley Periodicals, Inc. Environ Toxicol 31: 1389-1398, 2016.\"\n",
    "]\n",
    "\n",
    "# Example: load from file\n",
    "# passages = load_passages_from_file(\"/path/to/passages.txt\", limit=100)\n",
    "\n",
    "len(passages), passages[0][:80]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c561c0",
   "metadata": {},
   "source": [
    "## 8) Run inference (GO / HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7302a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(passages: List[str], do_go: bool = True, do_hpo: bool = True) -> Dict[str, List[dict]]:\n",
    "    out: Dict[str, List[dict]] = {}\n",
    "    if do_go:\n",
    "        if go_tagger is None:\n",
    "            raise RuntimeError(\"GO tagger is not initialized. Check GO_* config and catalog path.\")\n",
    "        out[\"go\"] = go_tagger.predict(passages)\n",
    "\n",
    "    if do_hpo:\n",
    "        if hpo_tagger is None:\n",
    "            raise RuntimeError(\"HPO tagger is not initialized. Check HPO_* config and catalog path.\")\n",
    "        out[\"hpo\"] = hpo_tagger.predict(passages)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "results = run_inference(passages, do_go=(go_tagger is not None), do_hpo=(hpo_tagger is not None))\n",
    "results.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf92843",
   "metadata": {},
   "source": [
    "## 9) Pretty-print results for the first passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a115cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top(results: Dict[str, List[dict]], idx: int = 0, topn: int = 10) -> None:\n",
    "    for space in [\"go\", \"hpo\"]:\n",
    "        if space not in results:\n",
    "            continue\n",
    "        ex = results[space][idx]\n",
    "        print(f\"\\n=== {space.upper()} ===\")\n",
    "        print(\"Passage:\", ex[\"query_text\"][:200] + (\"...\" if len(ex[\"query_text\"]) > 200 else \"\"))\n",
    "        for it in ex[\"items\"][:topn]:\n",
    "            print(f\"  {it['rank']:>2}  {it['score']:.4f}  {it['term_id']}  {it['term_text']}\")\n",
    "\n",
    "\n",
    "print_top(results, idx=0, topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81291bd3",
   "metadata": {},
   "source": [
    "## 10) Save outputs to JSONL\n",
    "\n",
    "We save:\n",
    "- `pred_go.jsonl` and/or `pred_hpo.jsonl`\n",
    "\n",
    "Each line:\n",
    "```json\n",
    "{\"query_text\": \"...\", \"items\": [{\"rank\":1,\"score\":...,\"term_id\":\"...\",\"term_text\":\"...\"}, ...]}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(path: Union[str, Path], rows: Iterable[dict]) -> None:\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "out_dir = Path(\"./inference_outputs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if \"go\" in results:\n",
    "    write_jsonl(out_dir / \"pred_go.jsonl\", results[\"go\"])\n",
    "if \"hpo\" in results:\n",
    "    write_jsonl(out_dir / \"pred_hpo.jsonl\", results[\"hpo\"])\n",
    "\n",
    "list(out_dir.glob(\"*.jsonl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ae7b5",
   "metadata": {},
   "source": [
    "## Optional: combined view (GO + HPO per passage)\n",
    "\n",
    "This is sometimes nicer for downstream UI/inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca12ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_go_hpo(go_rows: Optional[List[dict]], hpo_rows: Optional[List[dict]]) -> List[dict]:\n",
    "    n = 0\n",
    "    if go_rows is not None:\n",
    "        n = max(n, len(go_rows))\n",
    "    if hpo_rows is not None:\n",
    "        n = max(n, len(hpo_rows))\n",
    "\n",
    "    out: List[dict] = []\n",
    "    for i in range(n):\n",
    "        rec: Dict[str, Any] = {}\n",
    "        if go_rows is not None:\n",
    "            rec[\"query_text\"] = go_rows[i][\"query_text\"]\n",
    "            rec[\"go_items\"] = go_rows[i][\"items\"]\n",
    "        if hpo_rows is not None:\n",
    "            rec.setdefault(\"query_text\", hpo_rows[i][\"query_text\"])\n",
    "            rec[\"hpo_items\"] = hpo_rows[i][\"items\"]\n",
    "        out.append(rec)\n",
    "    return out\n",
    "\n",
    "\n",
    "combined = merge_go_hpo(results.get(\"go\"), results.get(\"hpo\"))\n",
    "combined[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
